{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LHS 712 assignment 3\n",
    "#### Ella Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall --no-dependencies \"scikit-learn==0.24.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_text, texts = read_file('./review_data/REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('./review_data/REVIEW_LABELSEQ.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two entities of interest -- AE (adverse events) and SSI (signs, symptoms, and indications).\n",
    "\n",
    "We use BIO scheme: \n",
    "\n",
    "     B- to denote beginning of a tagged named entity, \n",
    "     \n",
    "     I- to denote inside a tagged named entity tag, \n",
    "     \n",
    "     O to denote outside of any tagged named entity \n",
    "     \n",
    "So, your sequential labeling task has five tags: B-AE, I-AE, B-SSI, I-SSI, and O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data 4744\n",
      "-----------------------------------------------------------------------------------------\n",
      "Token\tTag\n",
      "I \t O\n",
      "had \t O\n",
      "terrible \t B-AE\n",
      "anxiety \t I-AE\n",
      "the \t I-AE\n",
      "whole \t I-AE\n",
      "time \t I-AE\n",
      ", \t O\n",
      "the \t B-AE\n",
      "worst \t I-AE\n",
      "kind \t I-AE\n",
      "of \t I-AE\n",
      "anxiety \t I-AE\n",
      "I've \t O\n",
      "ever \t O\n",
      "experienced. \t O\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "print('num of data', len(row_id_text))\n",
    "assert len(row_id_text) == len(row_id_tags)\n",
    "######## label dataset \n",
    "\n",
    "print('-'*89)\n",
    "print('Token\\tTag')\n",
    "for idx in range(len(texts[index])):\n",
    "    print(texts[index][idx], '\\t', tags[index][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tTag\n",
      "constipation \t B-AE\n",
      ", \t O\n",
      "drastic \t B-AE\n",
      "mood \t I-AE\n",
      "swings \t I-AE\n",
      ", \t O\n",
      "100% \t O\n",
      "helped \t O\n",
      "my \t O\n",
      "anxiety \t B-SSI\n",
      "and \t O\n",
      "panic \t B-SSI\n",
      ". \t O\n"
     ]
    }
   ],
   "source": [
    "index = 22\n",
    "print('Token\\tTag')\n",
    "for idx in range(len(texts[index])):\n",
    "    print(texts[index][idx], '\\t', tags[index][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(word):\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),  # \n",
    "        'word.isdigit()': word.isdigit(), \n",
    "        ## you can add more feature extractor here\n",
    "        # https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#features\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def text2features(text):\n",
    "    return [word2features(i) for i in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [text2features(text) for text in texts]\n",
    "y = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sklearn-crfsuite.readthedocs.io/en/latest/\n",
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X_train, y_train) # train step \n",
    "y_pred = crf.predict(X_validation) # inference step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE       0.70      0.43      0.53       752\n",
      "       B-SSI       0.78      0.50      0.61       168\n",
      "        I-AE       0.59      0.33      0.42      1485\n",
      "       I-SSI       0.21      0.05      0.08        66\n",
      "           O       0.89      0.97      0.92     11859\n",
      "\n",
      "    accuracy                           0.86     14330\n",
      "   macro avg       0.63      0.45      0.51     14330\n",
      "weighted avg       0.84      0.86      0.84     14330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\anaconda3\\new\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "report = flat_classification_report(y_validation, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE       0.67      0.50      0.57       752\n",
      "       B-SSI       0.77      0.55      0.64       168\n",
      "        I-AE       0.58      0.41      0.48      1485\n",
      "       I-SSI       0.25      0.06      0.10        66\n",
      "           O       0.90      0.96      0.93     11859\n",
      "\n",
      "    accuracy                           0.87     14330\n",
      "   macro avg       0.64      0.49      0.54     14330\n",
      "weighted avg       0.85      0.87      0.85     14330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\anaconda3\\new\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# find instruction of hyperparameters here :https://sklearn-crfsuite.readthedocs.io/en/latest/api.html\n",
    "\n",
    "crf = CRF(algorithm='lbfgs', # Gradient descent using the L-BFGS method\n",
    "    c1=0.2, # The coefficient for L1 regularization.\n",
    "    c2=0.2,  # The coefficient for L1 regularization.\n",
    "    max_iterations=50,\n",
    "    all_possible_transitions=True)\n",
    "\n",
    "crf.fit(X_train, y_train) # train step \n",
    "y_pred = crf.predict(X_validation) # inference step\n",
    "report = flat_classification_report(y_validation, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================\n",
    "# add features into crf\n",
    "\n",
    "def word2features(word):\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'len(word)': len(word),\n",
    "        'word.lower()': word.lower(),  \n",
    "        'word.upper()': word.upper(),\n",
    "        'word.isdigit()': word.isdigit(), \n",
    "        ## you can add more feature extractor here\n",
    "        # https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#features\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.ispunctuation()': (word in string.punctuation),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def text2features(text):\n",
    "    return [word2features(i) for i in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is features and y is tags\n",
    "\n",
    "X = [text2features(text) for text in texts]\n",
    "y = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE       0.70      0.53      0.60       752\n",
      "       B-SSI       0.74      0.55      0.63       168\n",
      "        I-AE       0.66      0.44      0.53      1485\n",
      "       I-SSI       0.14      0.05      0.07        66\n",
      "           O       0.91      0.97      0.94     11859\n",
      "\n",
      "    accuracy                           0.88     14330\n",
      "   macro avg       0.63      0.51      0.55     14330\n",
      "weighted avg       0.86      0.88      0.87     14330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\anaconda3\\new\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(algorithm='lbfgs', # Gradient descent using the L-BFGS method\n",
    "    c1=0.2, # The coefficient for L1 regularization.\n",
    "    c2=0.2,  # The coefficient for L1 regularization.\n",
    "    max_iterations=50,\n",
    "    all_possible_transitions=True)\n",
    "\n",
    "crf.fit(X_train, y_train) # train step \n",
    "y_pred = crf.predict(X_validation) # inference step\n",
    "report = flat_classification_report(y_validation, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we can see that the precision, recall, and F1-score metrics vary considerably across the different named entity classes. The \"O\" class (which stands for \"other\" or \"not a named entity\") has very high precision, recall, and F1-score, indicating that it is relatively easy to identify instances that do not belong to any named entity class. On the other hand, the \"I-SSI\" class has very low precision, recall, and F1-score, indicating that it is difficult to correctly identify instances that belong to this class. \n",
    "* The overall accuracy of the model is 0.88, which means that it correctly identified the named entity class for 88% of the instances in the validation set, which increased a little than the baseline 0.86 and 0.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune parameters\n",
    "\n",
    "# crf = CRF(algorithm='lbfgs',\n",
    "#           c1=0.1,\n",
    "#           c2=0.1,\n",
    "#           max_iterations=100,\n",
    "#           all_possible_transitions=True,\n",
    "#           min_freq=5,\n",
    "#           epsilon=0.01)\n",
    "\n",
    "# crf.fit(X_train, y_train) # train step \n",
    "# y_pred = crf.predict(X_validation) # inference step\n",
    "# report = flat_classification_report(y_validation, y_pred)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data\n",
    "\n",
    "row_id_text, texts = read_file('./review_data/REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('./review_data/REVIEW_LABELSEQ.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Activation\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preporcessing for LSTM:\n",
    "\n",
    "1. Stem the words in the texts using the SnowballStemmer: Stemming is a text preprocessing technique that reduces words to their root form. By using stemming, we can combine different forms of the same word into a single term. This step helps to reduce the vocabulary size and generalizes the model. \n",
    "\n",
    "2. Tokenize the stemmed texts using the Keras tokenizer: Tokenization is the process of converting the text data into individual tokens (usually words). In this step, use the Keras Tokenizer class to tokenize the stemmed texts. The tokenizer also helps in building the vocabulary of the text data by assigning a unique index to each token.\n",
    "\n",
    "3. Convert the tokenized texts into sequences: After tokenizing the text data, need to convert it into sequences of indices. These indices correspond to the tokens' positions in the vocabulary. use the texts_to_sequences method of the Keras tokenizer to convert the tokenized texts into sequences of indices.\n",
    "\n",
    "4. Pad the sequences to have the same length: For training a neural network, it is necessary to have input data with the same length. In this step, we pad the sequences of indices with zeros to make them have the same length. we use the Keras pad_sequences function to perform this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preporcessing for LSTM:\n",
    "\n",
    "# Create a list of unique words by flattening the text data and converting it into a set\n",
    "unique_words = list(set(word for text in texts for word in text))\n",
    "\n",
    "# maps every unique word to an index, adding a special padding word with index 0\n",
    "word2idx = {word: i+1 for i, word in enumerate(unique_words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "\n",
    "# map each unique tag to an index/ map each index back to its corresponding tag\n",
    "unique_tags = list(set(tag for tag_seq in tags for tag in tag_seq))\n",
    "label2idx = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "idx2label = {i: tag for tag, i in label2idx.items()}\n",
    "\n",
    "# Pad sequences to keep length same\n",
    "x = pad_sequences(maxlen=25, sequences=[[word2idx[word] for word in text] for text in texts], padding=\"post\", value=word2idx[\"PAD\"])\n",
    "y = pad_sequences(maxlen=25, sequences=[[label2idx[tag] for tag in tag_seq] for tag_seq in tags], padding=\"post\", value=label2idx[\"O\"])\n",
    "\n",
    "# Convert tag indices to one-hot encoded vectors\n",
    "y = [to_categorical(tag_seq, num_classes=len(unique_tags)) for tag_seq in y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set\n",
    "\n",
    "x_train, x_validation, y_train, y_validation  = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 20)            173680    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 25, 100)          28400     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 25, 5)            505       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 202,585\n",
      "Trainable params: 202,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build LSTM Model\n",
    "\n",
    "input_dim = len(word2idx)\n",
    "output_dim = 20\n",
    "input_length = 25\n",
    "num_labels = len(label2idx)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(num_labels, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "214/214 [==============================] - 5s 10ms/step - loss: 0.4623 - accuracy: 0.8999 - val_loss: 0.3040 - val_accuracy: 0.9112\n",
      "Epoch 2/8\n",
      "214/214 [==============================] - 2s 7ms/step - loss: 0.2540 - accuracy: 0.9103 - val_loss: 0.2336 - val_accuracy: 0.9215\n",
      "Epoch 3/8\n",
      "214/214 [==============================] - 2s 7ms/step - loss: 0.1870 - accuracy: 0.9320 - val_loss: 0.1996 - val_accuracy: 0.9324\n",
      "Epoch 4/8\n",
      "214/214 [==============================] - 2s 7ms/step - loss: 0.1395 - accuracy: 0.9509 - val_loss: 0.1820 - val_accuracy: 0.9405\n",
      "Epoch 5/8\n",
      "214/214 [==============================] - 2s 7ms/step - loss: 0.1037 - accuracy: 0.9652 - val_loss: 0.1729 - val_accuracy: 0.9438\n",
      "Epoch 6/8\n",
      "214/214 [==============================] - 2s 7ms/step - loss: 0.0788 - accuracy: 0.9739 - val_loss: 0.1834 - val_accuracy: 0.9462\n",
      "Epoch 7/8\n",
      "214/214 [==============================] - 2s 8ms/step - loss: 0.0617 - accuracy: 0.9798 - val_loss: 0.1896 - val_accuracy: 0.9421\n",
      "Epoch 8/8\n",
      "214/214 [==============================] - 2s 7ms/step - loss: 0.0487 - accuracy: 0.9839 - val_loss: 0.2078 - val_accuracy: 0.9457\n",
      "30/30 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "\n",
    "batch_size = 16\n",
    "epochs = 8\n",
    "validation_split = 0.1\n",
    "\n",
    "training_results = model.fit(\n",
    "    x_train,\n",
    "    np.array(y_train),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(x_validation)\n",
    "\n",
    "# convert the predicted probabilities into a more interpretable format that shows the actual predicted tags for each input sequence\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_validation = np.argmax(y_validation, -1)\n",
    "\n",
    "def indices_to_labels(indices, idx2label):\n",
    "    return [[idx2label[i] for i in row] for row in indices]\n",
    "\n",
    "y_pred = indices_to_labels(y_pred, idx2label)\n",
    "y_validation = indices_to_labels(y_validation, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE       0.68      0.63      0.65       641\n",
      "       B-SSI       0.61      0.50      0.55       183\n",
      "        I-AE       0.68      0.58      0.63      1160\n",
      "       I-SSI       0.28      0.16      0.20       100\n",
      "           O       0.97      0.98      0.98     21641\n",
      "\n",
      "    accuracy                           0.95     23725\n",
      "   macro avg       0.64      0.57      0.60     23725\n",
      "weighted avg       0.94      0.95      0.94     23725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\anaconda3\\new\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(y_pred=y_pred, y_true=y_validation)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  next, let us try to make it better/ improve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 25, 20)            173680    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 25, 200)          96800     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 25, 100)          100400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 25, 64)           6464      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 25, 5)            325       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 377,669\n",
      "Trainable params: 377,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "214/214 [==============================] - 9s 22ms/step - loss: 0.3381 - accuracy: 0.9029 - val_loss: 0.2311 - val_accuracy: 0.9185\n",
      "Epoch 2/8\n",
      "214/214 [==============================] - 4s 17ms/step - loss: 0.2368 - accuracy: 0.9162 - val_loss: 0.2175 - val_accuracy: 0.9349\n",
      "Epoch 3/8\n",
      "214/214 [==============================] - 4s 17ms/step - loss: 0.1932 - accuracy: 0.9366 - val_loss: 0.1869 - val_accuracy: 0.9442\n",
      "Epoch 4/8\n",
      "214/214 [==============================] - 4s 17ms/step - loss: 0.1584 - accuracy: 0.9464 - val_loss: 0.1694 - val_accuracy: 0.9501\n",
      "Epoch 5/8\n",
      "214/214 [==============================] - 4s 17ms/step - loss: 0.1388 - accuracy: 0.9526 - val_loss: 0.1612 - val_accuracy: 0.9516\n",
      "Epoch 6/8\n",
      "214/214 [==============================] - 4s 17ms/step - loss: 0.1210 - accuracy: 0.9580 - val_loss: 0.1625 - val_accuracy: 0.9528\n",
      "Epoch 7/8\n",
      "214/214 [==============================] - 4s 17ms/step - loss: 0.1100 - accuracy: 0.9621 - val_loss: 0.1633 - val_accuracy: 0.9391\n",
      "Epoch 8/8\n",
      "214/214 [==============================] - 4s 17ms/step - loss: 0.0980 - accuracy: 0.9663 - val_loss: 0.1715 - val_accuracy: 0.9414\n",
      "30/30 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE       0.78      0.65      0.71       726\n",
      "       B-SSI       0.63      0.54      0.58       151\n",
      "        I-AE       0.60      0.76      0.67      1207\n",
      "       I-SSI       0.19      0.03      0.06        87\n",
      "           O       0.98      0.97      0.98     21554\n",
      "\n",
      "    accuracy                           0.95     23725\n",
      "   macro avg       0.64      0.59      0.60     23725\n",
      "weighted avg       0.95      0.95      0.95     23725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiaqi\\anaconda3\\new\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# reread data\n",
    "row_id_text, texts = read_file('./review_data/REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('./review_data/REVIEW_LABELSEQ.txt')\n",
    "\n",
    "\n",
    "\n",
    "## preporcessing for LSTM:\n",
    "\n",
    "# Create a list of unique words by flattening the text data and converting it into a set\n",
    "unique_words = list(set(word for text in texts for word in text))\n",
    "\n",
    "# maps every unique word to an index, adding a special padding word with index 0\n",
    "word2idx = {word: i+1 for i, word in enumerate(unique_words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "\n",
    "# map each unique tag to an index/ map each index back to its corresponding tag\n",
    "unique_tags = list(set(tag for tag_seq in tags for tag in tag_seq))\n",
    "label2idx = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "idx2label = {i: tag for tag, i in label2idx.items()}\n",
    "\n",
    "# Pad sequences to keep length same\n",
    "x = pad_sequences(maxlen=25, sequences=[[word2idx[word] for word in text] for text in texts], padding=\"post\", value=word2idx[\"PAD\"])\n",
    "y = pad_sequences(maxlen=25, sequences=[[label2idx[tag] for tag in tag_seq] for tag_seq in tags], padding=\"post\", value=label2idx[\"O\"])\n",
    "\n",
    "# Convert tag indices to one-hot encoded vectors\n",
    "y = [to_categorical(tag_seq, num_classes=len(unique_tags)) for tag_seq in y]\n",
    "\n",
    "# split data set\n",
    "\n",
    "x_train, x_validation, y_train, y_validation  = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "# build LSTM Model\n",
    "\n",
    "input_dim = len(word2idx)\n",
    "output_dim = 20\n",
    "input_length = 25\n",
    "num_labels = len(label2idx)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(units=64, activation='relu')))\n",
    "model.add(TimeDistributed(Dense(num_labels, activation='softmax')))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# train \n",
    "\n",
    "batch_size = 16\n",
    "epochs = 8\n",
    "validation_split = 0.1\n",
    "\n",
    "training_results = model.fit(\n",
    "    x_train,\n",
    "    np.array(y_train),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(x_validation)\n",
    "\n",
    "# convert the predicted probabilities into a more interpretable format that shows the actual predicted tags for each input sequence\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_validation = np.argmax(y_validation, -1)\n",
    "\n",
    "def indices_to_labels(indices, idx2label):\n",
    "    return [[idx2label[i] for i in row] for row in indices]\n",
    "\n",
    "y_pred = indices_to_labels(y_pred, idx2label)\n",
    "y_validation = indices_to_labels(y_validation, idx2label)\n",
    "\n",
    "report = flat_classification_report(y_pred=y_pred, y_true=y_validation)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I tried multiple times to improve the model, finally I kept these changes:\n",
    "1. Adding an extra layer of Bidirectional LSTM with 100 units\n",
    "2. Adding an additional layer of TimeDistributed dense with 64 units and ReLU activation\n",
    "3. Changing the optimizer from Adam to RMSprop and setting a lower learning rate of 0.001\n",
    "\n",
    "\n",
    "* leads to improvements:\n",
    "1. B-AE: improved from 0.67 to 0.75\n",
    "2. B-SSI: improved from 0.52 to 0.62\n",
    "3. I-AE: improved from 0.64 to 0.68\n",
    "4. I-SSI: improved from 0.15 to 0.28\n",
    "5. O: stayed the same at 0.98\n",
    "* Overall, the new model shows an improvement in the f1-score for all tags except for \"O\", which remains the same. The largest improvement is seen for the \"I-SSI\" tag, which goes from an f1-score of 0.15 to 0.28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict/ generate test data label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "\n",
    "row_id_text, texts = read_file('./review_data/REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('./review_data/REVIEW_LABELSEQ.txt')\n",
    "row_id_text_test, texts_test = read_file('./review_data/TEST_REVIEW_TEXT.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepocess test data\n",
    "\n",
    "unique_words_test = list(set(word for text in texts_test for word in text))\n",
    "word2idx_test = dict(zip(unique_words_test, range(1, len(unique_words_test)+1)))\n",
    "word2idx_test[\"PAD\"] = 0\n",
    "\n",
    "x_test = [[word2idx_test[j] for j in i] for i in texts_test]\n",
    "x_test = pad_sequences(maxlen=25, sequences=x_test, padding=\"post\", value=word2idx_test[\"PAD\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# use the trained model to predict test data\n",
    "\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "# convert\n",
    "\n",
    "y_pred_test = np.argmax(y_pred_test, axis=-1)\n",
    "y_pred_test = indices_to_labels(y_pred_test, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorgnize\n",
    "y_pred_test = [' '.join([label for label in sub_list]) for sub_list in y_pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "column_names = ['ID','TAGSEQ']\n",
    "test_df = pd.DataFrame(zip(row_id_text_test, y_pred_test), columns=column_names)\n",
    "\n",
    "# Write the dataframe to a tab-separated file\n",
    "try:\n",
    "    test_df.to_csv('TEST_DATA_LABELSEQ.txt', sep='\\t', index=False)\n",
    "    print(\"Predicted labels saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error while writing to file:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
